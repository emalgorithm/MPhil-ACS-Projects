{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/data-tagged/'\n",
    "classes = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath):\n",
    "    \"\"\"Given a file, returns a list of tokens for that file\"\"\"\n",
    "    x = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for l in f:\n",
    "            # Filter lines which consist only of new line operator\n",
    "            if l == '\\n':\n",
    "                continue\n",
    "            \n",
    "            token, pos_tagging = l.split('\\t')\n",
    "            x.append(token)\n",
    "    return x\n",
    "\n",
    "def preprocess_data(datapath, sentiment='POS'):\n",
    "    idx = 0\n",
    "    X = []\n",
    "    y = []\n",
    "    sentiment_value = 1 if sentiment == 'POS' else 0\n",
    "    \n",
    "    # For file in the folder\n",
    "    current_path = datapath + sentiment\n",
    "    for f in os.listdir(current_path):\n",
    "        x = process_file(current_path + '/' + f)\n",
    "        X.append(x)\n",
    "        y.append(sentiment_value)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def get_unigram_dictionary(X, cutoff=1):\n",
    "    token_counter = Counter(np.concatenate(X))\n",
    "    idx = 0\n",
    "    token_to_idx = {}\n",
    "    \n",
    "    for x in X:\n",
    "        for token in x:\n",
    "            if token_counter[token] >= cutoff and token not in token_to_idx:\n",
    "                token_to_idx[token] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return token_to_idx\n",
    "\n",
    "def get_bigram_dictionary(X, cutoff=1, token_to_idx={}):\n",
    "    X_bigram = []\n",
    "    for x in X:\n",
    "        X_bigram += [(x[i], x[i + 1]) for i, _ in enumerate(x) if i < len(x) - 1 ]\n",
    "\n",
    "    token_counter = Counter(X_bigram)\n",
    "    idx = len(token_to_idx)\n",
    "    \n",
    "    for x in X:\n",
    "        x_bigram = [(x[i], x[i + 1]) for i, _ in enumerate(x) if i < len(x) - 1 ]\n",
    "        for token in x_bigram:\n",
    "            if token_counter[token] >= cutoff and token not in token_to_idx:\n",
    "                token_to_idx[token] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return token_to_idx\n",
    "\n",
    "def get_dictionary(X, cutoff=1, unigram=True, bigram=False):\n",
    "    \"\"\"\n",
    "    Returns a dictionary which maps each token to its index in the feature space.\n",
    "    Tokens which appear less times than specified by the cutoff are discarded\n",
    "    \"\"\"\n",
    "    token_to_idx = {}\n",
    "    if unigram:\n",
    "        token_to_idx = get_unigram_dictionary(X, cutoff)\n",
    "    if bigram:\n",
    "        token_to_idx = get_bigram_dictionary(X, cutoff, token_to_idx)\n",
    "    \n",
    "    print(\"Generated {} features with cutoff of {}\".format(len(token_to_idx), cutoff))\n",
    "                    \n",
    "    return token_to_idx\n",
    "\n",
    "def featurize_data(X, token_to_idx):\n",
    "    \"\"\"Convert each sample from a list of tokens to a multinomial bag of words representation\"\"\"\n",
    "    X_unigram_and_bigram = []\n",
    "    for x in X:\n",
    "        X_unigram_and_bigram.append(x + [(x[i], x[i + 1]) for i, _ in enumerate(x) if i < len(x) - 1 ])\n",
    "        \n",
    "    X_feat = []\n",
    "    for x in X:\n",
    "        x_feat = np.zeros((len(token_to_idx)))\n",
    "        for token in x:\n",
    "            if token in token_to_idx:\n",
    "                x_feat[token_to_idx[token]] += 1\n",
    "        X_feat.append(x_feat)\n",
    "    \n",
    "    return X_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[('aa', 'bb'), ('aa', 'ccc'), ('aa', 'bb')], [('aa', 'ccc'), ('aa', 'bbb')]]\n",
    "# b = []\n",
    "\n",
    "# for l in a:\n",
    "#     for item in l:\n",
    "#         b.append(item)\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos, y_pos = preprocess_data(data_path, 'POS')\n",
    "X_neg, y_neg = preprocess_data(data_path, 'NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has size (1800,)\n",
      "y_train has size (1800,)\n",
      "\n",
      "X_test has size (200,)\n",
      "y_test has size (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_pos[:900] + X_neg[:900]\n",
    "y_train = y_pos[:900] + y_neg[:900]\n",
    "\n",
    "X_test = X_pos[900:] + X_neg[900:]\n",
    "y_test = y_pos[900:] + y_neg[900:]\n",
    "\n",
    "print(\"X_train has size {}\".format(np.array(X_train).shape))\n",
    "print(\"y_train has size {}\".format(np.array(y_train).shape))\n",
    "print()\n",
    "print(\"X_test has size {}\".format(np.array(X_test).shape))\n",
    "print(\"y_test has size {}\".format(np.array(y_test).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_pos[:90] + X_neg[:90]\n",
    "y_train = y_pos[:90] + y_neg[:90]\n",
    "\n",
    "X_test = X_pos[900:] + X_neg[900:]\n",
    "y_test = y_pos[900:] + y_neg[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 74720 features with cutoff of 1\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = get_dictionary(X_train, cutoff=1, unigram=False, bigram=True)\n",
    "\n",
    "X_train = featurize_data(X_train, token_to_idx)\n",
    "X_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes():\n",
    "    def __init__(self, classes, num_feat, smoothing_value=0):\n",
    "        # Number of features the model uses\n",
    "        self.num_feat = num_feat\n",
    "        # List of the classes\n",
    "        self.classes = classes\n",
    "        # Dictionary mapping each class to the prior probability p(C=c)\n",
    "        self.class_to_prior = {c: 0 for c in classes}\n",
    "        # self.class_to_feature_to_cond_prob[c][x] is used to store the estimate of the conditional probability p(X=x|C=c)\n",
    "        self.class_to_feature_to_cond_prob = {c: np.zeros((num_feat,)) for c in classes}\n",
    "        # A smoothing value of 0 is equivalent to no smoothing\n",
    "        self.smoothing_value = smoothing_value\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        y = np.array(y)\n",
    "        X = np.array(X)\n",
    "        # Computer priors\n",
    "        for c in y:\n",
    "            self.class_to_prior[c] += 1\n",
    "        self.class_to_prior.update({c: self.class_to_prior[c] / len(y) for c in self.classes})\n",
    "        \n",
    "        # Compute estimate of the conditional probability p(X=x|C=c)\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            features_frequencies = np.sum(X_c, axis=0)\n",
    "            self.class_to_feature_to_cond_prob[c] = (features_frequencies + self.smoothing_value) / sum(features_frequencies + self.smoothing_value)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.argmax(np.stack([self.compute_scores(X, c) for c in self.classes], axis=-1), axis=1)\n",
    "    \n",
    "    def compute_scores(self, X, c):\n",
    "        # If smoothing is not applied, some conditional probability will be zero and so we can't take the log of them\n",
    "        # To solve the problem, we change 0 to a very small number (10 ** -50)\n",
    "        adjusted_cond_prob = np.array([p if p != 0 else 10 ** -50 for p in self.class_to_feature_to_cond_prob[c]])\n",
    "        return np.log(self.class_to_prior[c]) + np.matmul(X, np.log(adjusted_cond_prob))\n",
    "\n",
    "#     def compute_score(self, x, c):\n",
    "#         # Compute score (unnormalized log probability) for given class\n",
    "#         return np.log(self.class_to_prior[c]) + np.dot(x, np.log(self.smooth(self.class_to_feature_to_cond_prob[c])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 ms, sys: 140 ms, total: 270 ms\n",
      "Wall time: 270 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = MultinomialNaiveBayes(classes, len(X_train[0]), smoothing_value=1)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.00% of sentences are correctly classified\n",
      "CPU times: user 204 ms, sys: 136 ms, total: 339 ms\n",
      "Wall time: 389 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = model.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.00% of sentences are correctly classified\n",
      "CPU times: user 173 ms, sys: 56 ms, total: 229 ms\n",
      "Wall time: 183 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "n_correct = 0\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
