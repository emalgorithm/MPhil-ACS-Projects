{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/data-tagged/'\n",
    "classes = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath):\n",
    "    \"\"\"Given a file, returns a list of tokens for that file\"\"\"\n",
    "    x = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for l in f:\n",
    "            # Filter lines which consist only of new line operator\n",
    "            if l == '\\n':\n",
    "                continue\n",
    "            \n",
    "            token, pos_tagging = l.split('\\t')\n",
    "            x.append(token)\n",
    "    return x\n",
    "\n",
    "def preprocess_data(datapath, sentiment='POS'):\n",
    "    idx = 0\n",
    "    X = []\n",
    "    y = []\n",
    "    sentiment_value = 1 if sentiment == 'POS' else 0\n",
    "    \n",
    "    # For file in the folder\n",
    "    current_path = datapath + sentiment\n",
    "    for f in os.listdir(current_path):\n",
    "        x = process_file(current_path + '/' + f)\n",
    "        X.append(x)\n",
    "        y.append(sentiment_value)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def get_dictionary(X):\n",
    "    \"\"\"Returns a dictionary which maps each token to its index in the feature space, along with a list of all features (tokens) in order\"\"\"\n",
    "    idx = 0\n",
    "    token_to_idx = {}\n",
    "    features = []\n",
    "    \n",
    "    for x in X:\n",
    "        for token in x:\n",
    "            if token not in token_to_idx:\n",
    "                token_to_idx[token] = idx\n",
    "                idx += 1\n",
    "                features.append(token)\n",
    "    \n",
    "    return token_to_idx, features\n",
    "\n",
    "def featurize_data(X, features, token_to_idx):\n",
    "    \"\"\"Convert each sample from a list of tokens to a multinomial bag of words representation\"\"\"\n",
    "    X_feat = []\n",
    "    for x in X:\n",
    "        x_feat = np.zeros((len(features)))\n",
    "        for token in x:\n",
    "            x_feat[token_to_idx[token]] += 1\n",
    "        X_feat.append(x_feat)\n",
    "    \n",
    "    return X_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos, y_pos = preprocess_data(data_path, 'POS')\n",
    "X_neg, y_neg = preprocess_data(data_path, 'NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X_pos + X_neg\n",
    "# y = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_to_idx, features = get_dictionary(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = featurize_data(X, features, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert(sum(X[0]) != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes():\n",
    "    def __init__(self, classes, num_feat, smoothing_value=0):\n",
    "        # Number of features the model uses\n",
    "        self.num_feat = num_feat\n",
    "        # List of the classes\n",
    "        self.classes = classes\n",
    "        # Dictionary mapping each class to the prior probability p(C=c)\n",
    "        self.class_to_prior = {c: 0 for c in classes}\n",
    "        # self.class_to_feature_to_cond_prob[c][x] is used to store the estimate of the conditional probability p(X=x|C=c)\n",
    "        self.class_to_feature_to_cond_prob = {c: np.zeros((num_feat,)) for c in classes}\n",
    "        # A smoothing value of 0 is equivalent to no smoothing\n",
    "        self.smoothing_value = smoothing_value\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        y = np.array(y)\n",
    "        X = np.array(X)\n",
    "        # Computer priors\n",
    "        for c in y:\n",
    "            self.class_to_prior[c] += 1\n",
    "        self.class_to_prior.update({c: self.class_to_prior[c] / len(y) for c in self.classes})\n",
    "        \n",
    "        # Compute estimate of the conditional probability p(X=x|C=c)\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            features_frequencies = np.sum(X_c, axis=0)\n",
    "            self.class_to_feature_to_cond_prob[c] = (features_frequencies + self.smoothing_value) / sum(features_frequencies + self.smoothing_value)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.argmax(np.stack([self.compute_scores(X, c) for c in self.classes], axis=-1), axis=1)\n",
    "    \n",
    "    def compute_scores(self, X, c):\n",
    "        return np.log(self.class_to_prior[c]) + np.matmul(X, np.log(self.class_to_feature_to_cond_prob[c]))\n",
    "            \n",
    "#     def compute_score(self, x, c):\n",
    "#         # Compute score (unnormalized log probability) for given class\n",
    "#         return np.log(self.class_to_prior[c]) + np.dot(x, np.log(self.smooth(self.class_to_feature_to_cond_prob[c])))\n",
    "    \n",
    "    def smooth(self, x, laplacian=False):\n",
    "        x[x == 0.0] = 10 ** -15\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X_pos + X_neg\n",
    "# y = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_to_idx, features = get_dictionary(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = featurize_data(X, features, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_pos + X_neg\n",
    "\n",
    "X_train = X_pos[:900] + X_neg[:900]\n",
    "y_train = y_pos[:900] + y_neg[:900]\n",
    "\n",
    "X_test = X_pos[900:] + X_neg[900:]\n",
    "y_test = y_pos[900:] + y_neg[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx, features = get_dictionary(X)\n",
    "\n",
    "X_train = featurize_data(X_train, features, token_to_idx)\n",
    "X_test = featurize_data(X_test, features, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 721 ms, sys: 730 ms, total: 1.45 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = MultinomialNaiveBayes(classes, len(features), smoothing_value=1)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.50% of sentences are correctly classified\n",
      "CPU times: user 81.6 ms, sys: 37.2 ms, total: 119 ms\n",
      "Wall time: 160 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = model.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.50% of sentences are correctly classified\n",
      "CPU times: user 740 ms, sys: 410 ms, total: 1.15 s\n",
      "Wall time: 971 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "n_correct = 0\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
