{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multinomial_naive_bayes import MultinomialNaiveBayes\n",
    "from util import preprocess_data, get_dictionary, featurize_data, sign_test, cross_validation\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/data-tagged/'\n",
    "classes = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos, y_pos = preprocess_data(data_path, 'POS')\n",
    "X_neg, y_neg = preprocess_data(data_path, 'NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_pos[:900] + X_neg[:900]\n",
    "y_train = y_pos[:900] + y_neg[:900]\n",
    "\n",
    "X_test = X_pos[900:] + X_neg[900:]\n",
    "y_test = y_pos[900:] + y_neg[900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model with Held Out Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = 0\n",
    "unigram_cutoff = 1\n",
    "bigram_cutoff = 7\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=smoothing)\n",
    "\n",
    "model.fit(X_feat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = model.predict(X_feat_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Test all combinations of the models on the held out test set\n",
    "    smoothings = [0, 1]\n",
    "    unigrams = [True, False]\n",
    "    bigrams = [False, True]\n",
    "    unigram_cutoffs = [1, 4]\n",
    "    bigram_cutoffs = [1, 7]\n",
    "    \n",
    "    for unigram in unigrams:\n",
    "        for bigram in bigrams:\n",
    "            for unigram_cutoff in unigram_cutoffs:\n",
    "                for bigram_cutoff in bigram_cutoffs:\n",
    "                    for smoothing in smoothings:\n",
    "                        if not unigram and not bigram:\n",
    "                            continue\n",
    "                        if not unigram and unigram_cutoff == 4:\n",
    "                            continue\n",
    "                        if not bigram and bigram_cutoff == 7:\n",
    "                            continue\n",
    "                        X_train = X_pos[:900] + X_neg[:900]\n",
    "                        y_train = y_pos[:900] + y_neg[:900]\n",
    "\n",
    "                        X_test = X_pos[900:] + X_neg[900:]\n",
    "                        y_test = y_pos[900:] + y_neg[900:]\n",
    "                        print(\"unigram: {}, bigram: {}, unigram_cutoff: {}, bigram_cutoff: {}, smoothing: {}\".format(unigram, bigram, unigram_cutoff, bigram_cutoff, smoothing))\n",
    "                        token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "                        X_train = featurize_data(X_train, token_to_idx)\n",
    "                        X_test = featurize_data(X_test, token_to_idx)\n",
    "\n",
    "                        model = MultinomialNaiveBayes(classes, len(X_train[0]), smoothing_value=smoothing)\n",
    "                        model.fit(X_train, y_train)\n",
    "                        y_pred = model.predict(X_test)\n",
    "                        n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "                        print(\"{0:.2f}% of sentences are correctly classified \\n\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign Test on Held Out Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare models that use smoothing with models that don't use smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 1\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=0)\n",
    "\n",
    "# Train models on the same data\n",
    "model1.fit(X_feat_train, y_train)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "\n",
    "# Test models\n",
    "y1_pred = model1.predict(X_feat_test)\n",
    "y2_pred = model2.predict(X_feat_test)\n",
    "\n",
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 1\n",
    "bigram_cutoff = 7\n",
    "unigram=False\n",
    "bigram=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=0)\n",
    "\n",
    "# Train models on the same data\n",
    "model1.fit(X_feat_train, y_train)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "\n",
    "# Test models\n",
    "y1_pred = model1.predict(X_feat_test)\n",
    "y2_pred = model2.predict(X_feat_test)\n",
    "\n",
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams + Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 7\n",
    "unigram=True\n",
    "bigram=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=0)\n",
    "\n",
    "# Train models on the same data\n",
    "model1.fit(X_feat_train, y_train)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "\n",
    "# Test models\n",
    "y1_pred = model1.predict(X_feat_test)\n",
    "y2_pred = model2.predict(X_feat_test)\n",
    "\n",
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams vs Bigrams both with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 1\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model1.fit(X_feat_train, y_train)\n",
    "y1_pred = model1.predict(X_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 1\n",
    "bigram_cutoff = 7\n",
    "unigram=False\n",
    "bigram=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "y2_pred = model2.predict(X_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams+ Bigrams vs Unigrams both with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 7\n",
    "unigram=True\n",
    "bigram=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model1.fit(X_feat_train, y_train)\n",
    "y1_pred = model1.predict(X_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 1\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "y2_pred = model2.predict(X_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate all models using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_cv(X_pos, X_neg, y_pos, y_neg):\n",
    "    # Run cross validation for all combinations of the models\n",
    "    smoothings = [0, 1]\n",
    "    unigrams = [True, False]\n",
    "    bigrams = [False, True]\n",
    "    unigram_cutoff = 4\n",
    "    bigram_cutoff = 7\n",
    "    \n",
    "    X = np.array(X_pos + X_neg)\n",
    "    y = np.array(y_pos + y_neg)\n",
    "    \n",
    "    for unigram in unigrams:\n",
    "        for bigram in bigrams:\n",
    "            for smoothing in smoothings:\n",
    "                if not unigram and not bigram:\n",
    "                    continue\n",
    "                print(\"unigram: {}, bigram: {}, unigram_cutoff: {}, bigram_cutoff: {}, smoothing: {}\".format(unigram, bigram, unigram_cutoff, bigram_cutoff, smoothing))\n",
    "\n",
    "                model = MultinomialNaiveBayes(classes, len(X[0]), smoothing_value=smoothing)\n",
    "                cross_validation(model, X, y, unigram=unigram, bigram=bigram, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_full_cv(X_pos, X_neg, y_pos, y_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign Test\n",
    "Run the sign test on cross validation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_pos + X_neg)\n",
    "y = np.array(y_pos + y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 1\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=0)\n",
    "\n",
    "y1_pred, y1_test = cross_validation(model1, X, y, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "y2_pred, y2_test = cross_validation(model2, X, y, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "assert(np.array_equal(y1_test, y2_test))\n",
    "sign_test(y1_pred, y2_pred, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
